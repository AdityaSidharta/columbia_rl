{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "auburn-pittsburgh",
   "metadata": {
    "id": "auburn-pittsburgh",
    "tags": []
   },
   "source": [
    "# Lab 1 - FrozenLake MDP: Part 1\n",
    "# Assignment\n",
    "\n",
    "- In this assignment you will learn how to tackle problems with limited state spaces.\n",
    "- In particular we consider the FrozenLake MDP problem.\n",
    "\n",
    "# Outline\n",
    "\n",
    "- Part 0 introduces us to [gym](https://gym.openai.com/), an environment that allows us to test our reinforcement learning algorithms in various problems\n",
    "- In Part 1, you will implement a policy iteration algorithm (HW1)\n",
    "- In Part 2, you will implement Q-Learning and SARSA (in next homework HW2) \n",
    "\n",
    "# Deliverable\n",
    "\n",
    "Regarding the Lab:\n",
    "\n",
    "- Make sure your code runs from top to bottom without any errors.\n",
    "- Your submitted Notebook must contain saved outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "EG9GXg2xixYf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 246,
     "status": "ok",
     "timestamp": 1644369412150,
     "user": {
      "displayName": "Aditya Sidharta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj-wonlf58m1hUVaWshoARe7aHldD1aYfVlSHcyArA=s64",
      "userId": "17627097548623080996"
     },
     "user_tz": 300
    },
    "id": "EG9GXg2xixYf",
    "outputId": "77e416b0-28a9-4d0f-d4d3-b664a21d8245"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: {os.sys.executable}: command not found\n",
      "/bin/bash: {os.sys.executable}: command not found\n"
     ]
    }
   ],
   "source": [
    "!{os.sys.executable} -m pip install numpy\n",
    "!{os.sys.executable} -m pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "flush-grove",
   "metadata": {
    "executionInfo": {
     "elapsed": 1024,
     "status": "ok",
     "timestamp": 1644369413359,
     "user": {
      "displayName": "Aditya Sidharta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj-wonlf58m1hUVaWshoARe7aHldD1aYfVlSHcyArA=s64",
      "userId": "17627097548623080996"
     },
     "user_tz": 300
    },
    "id": "flush-grove"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# You will need numpy and gym. You can try running the following lines to install them\n",
    "# The assignment is tested on Python3.8 so in case you are having installation issues you might \n",
    "# want to try installing that version. \n",
    "\n",
    "\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-dancing",
   "metadata": {
    "id": "medium-dancing"
   },
   "source": [
    "# Part 0 - Introduction to Gym\n",
    "- We look at [FrozenLake-v0 environment](https://gym.openai.com/envs/FrozenLake-v0/) in gym. \n",
    "- You don't need to write any code for this part\n",
    "- you should still understand the code to help you solve Part 1 and Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "considerable-swing",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 829,
     "status": "ok",
     "timestamp": 1644369414181,
     "user": {
      "displayName": "Aditya Sidharta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj-wonlf58m1hUVaWshoARe7aHldD1aYfVlSHcyArA=s64",
      "userId": "17627097548623080996"
     },
     "user_tz": 300
    },
    "id": "considerable-swing",
    "outputId": "04080501-ebbc-4090-fd78-b12f59c9cb45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of States 16, Number of Actions 4\n",
      "Reward range (0, 1)\n"
     ]
    }
   ],
   "source": [
    "# Import the environment we will use in this assignment\n",
    "env=gym.make('FrozenLake-v0') \n",
    "\n",
    "# Show the model\n",
    "print(f\"Number of States {env.nS}, Number of Actions {env.nA}\")\n",
    "print(f\"Reward range {env.reward_range}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "severe-conjunction",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 228,
     "status": "ok",
     "timestamp": 1644369415971,
     "user": {
      "displayName": "Aditya Sidharta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj-wonlf58m1hUVaWshoARe7aHldD1aYfVlSHcyArA=s64",
      "userId": "17627097548623080996"
     },
     "user_tz": 300
    },
    "id": "severe-conjunction",
    "outputId": "ecd3fdb2-94a3-4ac9-89ff-4f3e89c9d899"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset() # reset the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "endless-cassette",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1644369416728,
     "user": {
      "displayName": "Aditya Sidharta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj-wonlf58m1hUVaWshoARe7aHldD1aYfVlSHcyArA=s64",
      "userId": "17627097548623080996"
     },
     "user_tz": 300
    },
    "id": "endless-cassette",
    "outputId": "79ba6dd3-1f42-4e1d-b5f0-6f25ce1c16bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# visualize the current state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "naughty-bread",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1644369417952,
     "user": {
      "displayName": "Aditya Sidharta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj-wonlf58m1hUVaWshoARe7aHldD1aYfVlSHcyArA=s64",
      "userId": "17627097548623080996"
     },
     "user_tz": 300
    },
    "id": "naughty-bread",
    "outputId": "ae33994d-ff82-481c-9b2a-9719f49b8bc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Render State after 25 slots\n",
      "  (Up)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reached terminal state? True\n"
     ]
    }
   ],
   "source": [
    "# run a policy that chooses actions randomly \n",
    "env.reset()\n",
    "n = 25\n",
    "for i in range(n):\n",
    "    a = env.action_space.sample() # Sample Random Action\n",
    "    state, reward, finished, _ = env.step(a)\n",
    "    if finished: break\n",
    "        \n",
    "print(f'Render State after {n} slots')\n",
    "env.render()\n",
    "print(f'Reached terminal state? {finished}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "front-nomination",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1644369420099,
     "user": {
      "displayName": "Aditya Sidharta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj-wonlf58m1hUVaWshoARe7aHldD1aYfVlSHcyArA=s64",
      "userId": "17627097548623080996"
     },
     "user_tz": 300
    },
    "id": "front-nomination",
    "outputId": "1a3ab2b1-159c-4f20-f45d-236b04711611"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset() # Let's reset the state again\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-china",
   "metadata": {
    "id": "official-china"
   },
   "source": [
    "# Part 1 - MDP and Planning: Implement Policy Iteration \n",
    "- In this part we will focus on methods that assume knowledge of the enivonment dynamics, in partucular you will implement Policy Iteration. \n",
    "- The environment model can be obtained through `env.P`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4QHPHXQOkT2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1644369421887,
     "user": {
      "displayName": "Aditya Sidharta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj-wonlf58m1hUVaWshoARe7aHldD1aYfVlSHcyArA=s64",
      "userId": "17627097548623080996"
     },
     "user_tz": 300
    },
    "id": "4QHPHXQOkT2b",
    "outputId": "ae82df71-656b-4ae8-9f7e-1aab9a1d9208"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(0.3333333333333333, 10, 0.0, False),\n",
       "  (0.3333333333333333, 13, 0.0, False),\n",
       "  (0.3333333333333333, 14, 0.0, False)],\n",
       " 1: [(0.3333333333333333, 13, 0.0, False),\n",
       "  (0.3333333333333333, 14, 0.0, False),\n",
       "  (0.3333333333333333, 15, 1.0, True)],\n",
       " 2: [(0.3333333333333333, 14, 0.0, False),\n",
       "  (0.3333333333333333, 15, 1.0, True),\n",
       "  (0.3333333333333333, 10, 0.0, False)],\n",
       " 3: [(0.3333333333333333, 15, 1.0, True),\n",
       "  (0.3333333333333333, 10, 0.0, False),\n",
       "  (0.3333333333333333, 13, 0.0, False)]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "spanish-feeding",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1644369423338,
     "user": {
      "displayName": "Aditya Sidharta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj-wonlf58m1hUVaWshoARe7aHldD1aYfVlSHcyArA=s64",
      "userId": "17627097548623080996"
     },
     "user_tz": 300
    },
    "id": "spanish-feeding",
    "outputId": "02236d06-bcc7-443d-d5f9-7dfa05818739"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 2, 0.0, False),\n",
       " (0.3333333333333333, 1, 0.0, False),\n",
       " (0.3333333333333333, 0, 0.0, False)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No need to change anything here. Try to understand what happens \n",
    "\n",
    "# let's look at a random state-action pair and observe its transition characteristics\n",
    "# you can re-run this cell to get a different state-action pair\n",
    "random_state  = env.observation_space.sample()\n",
    "random_action = env.action_space.sample()\n",
    "# returns a list of tuples (probability,newstate,reward,is_terminal_state)\n",
    "env.P[random_state][random_action] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "lucky-place",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1644369514872,
     "user": {
      "displayName": "Aditya Sidharta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj-wonlf58m1hUVaWshoARe7aHldD1aYfVlSHcyArA=s64",
      "userId": "17627097548623080996"
     },
     "user_tz": 300
    },
    "id": "lucky-place",
    "outputId": "ad96ee95-2f09-40c6-a2ed-6a0e1d5e2aee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 7, 11, 12, 15]\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# YOUR CODE HERE #\n",
    "# Print all the terminal states in the environment.\n",
    "# you can use env.P\n",
    "############################\n",
    "terminal_states = []\n",
    "for s in range(env.nS):\n",
    "    for a in range(env.nA):\n",
    "        for prob, next_state, reward, isend in env.P[s][a]:\n",
    "            if isend:\n",
    "                terminal_states.append(next_state)\n",
    "print(list(set(terminal_states)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "rocky-danger",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 145,
     "status": "ok",
     "timestamp": 1644369536304,
     "user": {
      "displayName": "Aditya Sidharta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj-wonlf58m1hUVaWshoARe7aHldD1aYfVlSHcyArA=s64",
      "userId": "17627097548623080996"
     },
     "user_tz": 300
    },
    "id": "rocky-danger",
    "outputId": "80d7e083-3f0d-4fe0-c20b-c8b0e97146e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Verify your solution (look at the positions where final states are)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-pendant",
   "metadata": {
    "id": "protective-pendant"
   },
   "source": [
    "\n",
    "### Step A: Implement Policy Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "forty-offering",
   "metadata": {
    "executionInfo": {
     "elapsed": 149,
     "status": "ok",
     "timestamp": 1644369539571,
     "user": {
      "displayName": "Aditya Sidharta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj-wonlf58m1hUVaWshoARe7aHldD1aYfVlSHcyArA=s64",
      "userId": "17627097548623080996"
     },
     "user_tz": 300
    },
    "id": "forty-offering"
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(P, nS, nA, policy, gamma=0.9, tol=1e-3):\n",
    "    \"\"\"Evaluate the value function from a given policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    policy: np.array[nS]\n",
    "        The policy to evaluate. Maps states to actions.\n",
    "    tol: float\n",
    "        Terminate policy evaluation when\n",
    "            max |value_function(s) - prev_value_function(s)| < tol\n",
    "    Returns\n",
    "    -------\n",
    "    value_function: np.ndarray[nS]\n",
    "        The value function of the given policy, where value_function[s] is\n",
    "        the value of state s\n",
    "    \"\"\"\n",
    "    value_function = np.zeros(nS)          \n",
    "            \n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "\n",
    "    ############################\n",
    "    def calc_vs(P, policy, value_function, gamma, cur_state):\n",
    "        result = 0\n",
    "        total_prob = 0\n",
    "        action = policy[cur_state]\n",
    "        for (prob, next_state, reward, isend) in P[cur_state][action]:\n",
    "            result = result + prob * (reward + gamma * (value_function[next_state]))\n",
    "            total_prob = total_prob + prob\n",
    "        assert total_prob == 1\n",
    "        return result\n",
    "\n",
    "    converge = False\n",
    "    while not converge:\n",
    "        cur_tol = 0\n",
    "        for s in range(nS):\n",
    "            previous_value = value_function[s]\n",
    "            value_function[s] = calc_vs(P, policy, value_function, gamma, s)\n",
    "            cur_tol = max(cur_tol, abs(value_function[s] - previous_value))\n",
    "        if cur_tol < tol:\n",
    "            converge = True\n",
    "\n",
    "    return value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-property",
   "metadata": {
    "id": "stunning-property"
   },
   "source": [
    "#### Evaluate random policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "naked-tooth",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1644369542240,
     "user": {
      "displayName": "Aditya Sidharta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj-wonlf58m1hUVaWshoARe7aHldD1aYfVlSHcyArA=s64",
      "userId": "17627097548623080996"
     },
     "user_tz": 300
    },
    "id": "naked-tooth",
    "outputId": "b4b72625-d471-4498-d665-6a1db315105f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Policy 0 ------------------------------\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "-------- Policy 1 ------------------------------\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "-------- Policy 2 ------------------------------\n",
      "[0.00883696 0.00425529 0.001908   0.00076179 0.01824949 0.\n",
      " 0.0005724  0.         0.0344539  0.08058255 0.16411957 0.\n",
      " 0.         0.23419423 0.54651705 0.        ]\n",
      "-------- Policy 3 ------------------------------\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "-------- Policy 4 ------------------------------\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Test your policy_evaluation on 5 randomly generated deterministic policies\n",
    "# print the value function of the policies\n",
    "\n",
    "############################\n",
    "# YOUR CODE HERE #\n",
    "\n",
    "for i in range(5):\n",
    "   random_policy = np.random.randint(low=0, high=env.nA, size=env.nS)\n",
    "   print(f'-------- Policy {i}','-'*30)\n",
    "   print(policy_evaluation(env.P, env.nS, env.nA, random_policy))\n",
    "\n",
    "############################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-evaluation",
   "metadata": {
    "id": "ongoing-evaluation"
   },
   "source": [
    "### Step B: Implement Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cooperative-dressing",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1644369548449,
     "user": {
      "displayName": "Aditya Sidharta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj-wonlf58m1hUVaWshoARe7aHldD1aYfVlSHcyArA=s64",
      "userId": "17627097548623080996"
     },
     "user_tz": 300
    },
    "id": "cooperative-dressing"
   },
   "outputs": [],
   "source": [
    "def policy_improvement(P, nS, nA, value_from_policy, policy, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Given the value function from policy improve the policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    value_from_policy: np.ndarray\n",
    "        The value calculated from the policy\n",
    "    policy: np.array\n",
    "        The previous policy.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new_policy: np.ndarray[nS]\n",
    "        An array of integers. Each integer is the optimal action to take\n",
    "        in that state according to the environment dynamics and the\n",
    "        given value function.\n",
    "    \"\"\"\n",
    "    new_policy = np.zeros(nS, dtype='int')\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    ############################\n",
    "    def calc_qs(P, value_function, action, gamma, cur_state):\n",
    "        result = 0\n",
    "        total_prob = 0\n",
    "        for (prob, next_state, reward, isend) in P[cur_state][action]:\n",
    "            result = result + prob * (reward + gamma * (value_function[next_state]))\n",
    "            total_prob = total_prob + prob\n",
    "        assert total_prob == 1\n",
    "        return result\n",
    "\n",
    "    converge = False\n",
    "    old_policy = policy.copy()\n",
    "    while not converge:\n",
    "        for s in range(nS):\n",
    "            qs = np.zeros(nA, dtype='float')\n",
    "            for a in range(nA):\n",
    "                qs[a] = calc_qs(P, value_from_policy, a, gamma, s)\n",
    "            new_policy[s] = np.argmax(qs)\n",
    "        if np.all(old_policy == new_policy):\n",
    "            converge = True\n",
    "        else:\n",
    "            old_policy = new_policy\n",
    "\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "behavioral-penetration",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1644369548451,
     "user": {
      "displayName": "Aditya Sidharta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj-wonlf58m1hUVaWshoARe7aHldD1aYfVlSHcyArA=s64",
      "userId": "17627097548623080996"
     },
     "user_tz": 300
    },
    "id": "behavioral-penetration",
    "outputId": "ecdd16a0-04be-44ec-e06e-8a5a4c76d21b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Policy 0 ------------------------------\n",
      "Policy : [0 3 2 3 3 1 2 3 2 0 2 2 0 0 1 0]\n",
      "Value Function: [0.         0.01449378 0.03497827 0.02487401 0.         0.\n",
      " 0.05860305 0.         0.         0.         0.16043798 0.\n",
      " 0.         0.         0.4761904  0.        ]\n",
      "Mean : 0.04809859299338834\n",
      "\n",
      "-------> Policy 0 IMPROVED ---------------------\n",
      "Policy : [1 2 2 3 0 0 0 0 0 1 0 0 0 1 2 0]\n",
      "Value Function: [0.01203756 0.02316454 0.05516846 0.04008759 0.00606778 0.\n",
      " 0.09045277 0.         0.00254226 0.14943666 0.24648406 0.\n",
      " 0.         0.24928898 0.58179579 0.        ]\n",
      "Mean : 0.09103290282136381\n",
      "\n",
      "-------- Policy 1 ------------------------------\n",
      "Policy : [2 1 2 0 2 3 2 3 0 0 0 1 2 0 1 1]\n",
      "Value Function: [4.24004116e-03 9.75342818e-03 2.89427722e-02 1.22320477e-02\n",
      " 1.41109920e-03 0.00000000e+00 5.66255857e-02 0.00000000e+00\n",
      " 5.62416619e-04 1.85751289e-04 1.59904063e-01 0.00000000e+00\n",
      " 0.00000000e+00 7.27516894e-05 4.76215820e-01 0.00000000e+00]\n",
      "Mean : 0.04688411105372361\n",
      "\n",
      "-------> Policy 1 IMPROVED ---------------------\n",
      "Policy : [3 3 2 3 0 0 0 0 3 1 0 0 0 2 2 0]\n",
      "Value Function: [0.02636419 0.03771108 0.06284707 0.04625063 0.06354825 0.\n",
      " 0.10170821 0.         0.12330606 0.22512184 0.27639755 0.\n",
      " 0.         0.35116102 0.59460114 0.        ]\n",
      "Mean : 0.11931356520125089\n",
      "\n",
      "-------- Policy 2 ------------------------------\n",
      "Policy : [3 0 0 1 0 3 1 0 1 2 2 0 3 0 1 0]\n",
      "Value Function: [0.         0.         0.02043242 0.00866822 0.00951986 0.\n",
      " 0.04801705 0.         0.02328837 0.05500356 0.16020563 0.\n",
      " 0.         0.02348758 0.48618012 0.        ]\n",
      "Mean : 0.05217517511179076\n",
      "\n",
      "-------> Policy 2 IMPROVED ---------------------\n",
      "Policy : [0 1 2 3 0 0 0 0 3 1 0 0 0 2 2 0]\n",
      "Value Function: [0.06001474 0.0372182  0.06426294 0.04781444 0.08239675 0.\n",
      " 0.10271565 0.         0.13334637 0.22942123 0.27824745 0.\n",
      " 0.         0.3534133  0.59541395 0.        ]\n",
      "Mean : 0.1240165629416162\n",
      "\n",
      "-------- Policy 3 ------------------------------\n",
      "Policy : [1 3 3 1 1 3 1 1 3 2 0 3 0 0 1 1]\n",
      "Value Function: [5.68323402e-03 2.54103103e-03 1.09064105e-03 4.15199915e-04\n",
      " 1.29869998e-02 0.00000000e+00 5.43081291e-02 0.00000000e+00\n",
      " 3.18350342e-02 6.22449285e-02 1.81192354e-01 0.00000000e+00\n",
      " 0.00000000e+00 2.66102780e-02 4.87542853e-01 0.00000000e+00]\n",
      "Mean : 0.05415316764706918\n",
      "\n",
      "-------> Policy 3 IMPROVED ---------------------\n",
      "Policy : [0 3 0 3 0 0 0 0 3 1 0 0 0 2 2 0]\n",
      "Value Function: [0.06002718 0.05420982 0.06747758 0.04998592 0.08242857 0.\n",
      " 0.1038008  0.         0.13342631 0.22960489 0.27867801 0.\n",
      " 0.         0.35355723 0.59559304 0.        ]\n",
      "Mean : 0.12554933564859966\n",
      "\n",
      "-------- Policy 4 ------------------------------\n",
      "Policy : [1 1 2 1 2 2 3 3 2 2 1 3 2 3 0 0]\n",
      "Value Function: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Mean : 0.0\n",
      "\n",
      "-------> Policy 4 IMPROVED ---------------------\n",
      "Policy : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "Value Function: [0.         0.         0.02279889 0.00950109 0.         0.\n",
      " 0.05453427 0.         0.         0.         0.15918618 0.\n",
      " 0.         0.         0.47615923 0.        ]\n",
      "Mean : 0.04513622946978333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the value before and after policy improvements for 5 randomly generated policies\n",
    "\n",
    "############################\n",
    "# YOUR CODE HERE #\n",
    "############################\n",
    "\n",
    "for i in range(5):\n",
    "   random_policy = np.random.randint(low=0, high=env.nA, size=env.nS)\n",
    "   print(f'-------- Policy {i}','-'*30)\n",
    "   prev_value_function = policy_evaluation(env.P, env.nS, env.nA, random_policy)\n",
    "   print(\"Policy : {}\".format(random_policy))\n",
    "   print(\"Value Function: {}\".format(prev_value_function))\n",
    "   print(\"Mean : {}\".format(prev_value_function.mean()))\n",
    "   print() # Print value function before improvement\n",
    "\n",
    "   improved_policy = policy_improvement(env.P, env.nS, env.nA, prev_value_function, random_policy)\n",
    "   improved_value_function = policy_evaluation(env.P, env.nS, env.nA, improved_policy)\n",
    "   print(f'-------> Policy {i} IMPROVED','-'*21)\n",
    "   print(\"Policy : {}\".format(improved_policy))\n",
    "   print(\"Value Function: {}\".format(improved_value_function))\n",
    "   print(\"Mean : {}\".format(improved_value_function.mean()))\n",
    "   print() # Print value function before improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-tragedy",
   "metadata": {
    "id": "independent-tragedy"
   },
   "source": [
    "### Step C: Implement Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ranking-template",
   "metadata": {
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1644369562556,
     "user": {
      "displayName": "Aditya Sidharta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj-wonlf58m1hUVaWshoARe7aHldD1aYfVlSHcyArA=s64",
      "userId": "17627097548623080996"
     },
     "user_tz": 300
    },
    "id": "ranking-template"
   },
   "outputs": [],
   "source": [
    "def policy_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n",
    "    \"\"\" \n",
    "    Run policy iteration for dynamics of P.\n",
    "\n",
    "    You should use your methods: policy_evaluation() and policy_improvement() here\n",
    "\n",
    "    Parameters: \n",
    "    P, nS, nA, gamma: defined at beginning of file\n",
    "    tolerance:        tolerance parameter used in policy_evaluation()\n",
    "    \n",
    "    Returns: \n",
    "    value_function: np.ndarray[nS]\n",
    "    policy:         np.ndarray[nS]\n",
    "    \"\"\"\n",
    "\n",
    "    value_function = np.zeros(nS)\n",
    "    policy = np.zeros(nS, dtype=int)\n",
    "\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    ############################\n",
    "    converge = False\n",
    "    policy = np.random.randint(low=0, high=nA, size=nS)\n",
    "    prev_policy = policy.copy()\n",
    "    while not converge:\n",
    "        value_function = policy_evaluation(P, nS, nA, policy, gamma, tol)\n",
    "        policy = policy_improvement(P, nS, nA, value_function, policy, gamma)\n",
    "        if np.all(prev_policy == policy):\n",
    "            converge = True\n",
    "        else:\n",
    "            prev_policy = policy.copy()\n",
    "    return value_function, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "KKXTCJOd8XM2",
   "metadata": {
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1644369563777,
     "user": {
      "displayName": "Aditya Sidharta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj-wonlf58m1hUVaWshoARe7aHldD1aYfVlSHcyArA=s64",
      "userId": "17627097548623080996"
     },
     "user_tz": 300
    },
    "id": "KKXTCJOd8XM2"
   },
   "outputs": [],
   "source": [
    "def test_policy(policy, n_trials = 100):\n",
    "    env = gym.make(\"FrozenLake-v0\")\n",
    "    env.reset()\n",
    "    success = 0\n",
    "\n",
    "    for _ in range(n_trials):\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "            state, reward, done, _ = env.step(action)\n",
    "        success = success + reward\n",
    "\n",
    "    avg_success_rate = success / n_trials\n",
    "    return avg_success_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "gfRbPnQs85E-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1644369564635,
     "user": {
      "displayName": "Aditya Sidharta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj-wonlf58m1hUVaWshoARe7aHldD1aYfVlSHcyArA=s64",
      "userId": "17627097548623080996"
     },
     "user_tz": 300
    },
    "id": "gfRbPnQs85E-",
    "outputId": "2616fcdf-a969-40fa-c3cc-a5b15be8701c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_policy = np.random.randint(low=0, high=env.nA, size=env.nS)\n",
    "test_policy(random_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "zv0wa0ja9eJp",
   "metadata": {
    "executionInfo": {
     "elapsed": 157,
     "status": "ok",
     "timestamp": 1644369566170,
     "user": {
      "displayName": "Aditya Sidharta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj-wonlf58m1hUVaWshoARe7aHldD1aYfVlSHcyArA=s64",
      "userId": "17627097548623080996"
     },
     "user_tz": 300
    },
    "id": "zv0wa0ja9eJp"
   },
   "outputs": [],
   "source": [
    "def visualize_policy(policy):\n",
    "    vpolicy = policy.copy()\n",
    "    mapper = {0: 'left', 1: 'down', 2: 'right', 3: 'up'}\n",
    "    vpolicy = [mapper[x] for x in vpolicy.tolist()]\n",
    "    print(vpolicy[0:4])\n",
    "    print(vpolicy[4:8])\n",
    "    print(vpolicy[8:12])\n",
    "    print(vpolicy[12:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-cookie",
   "metadata": {
    "id": "individual-cookie"
   },
   "source": [
    "#### Call your function for gamma=0.9 and gamma=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5r_h38uj-WL4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 187,
     "status": "ok",
     "timestamp": 1644369571664,
     "user": {
      "displayName": "Aditya Sidharta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj-wonlf58m1hUVaWshoARe7aHldD1aYfVlSHcyArA=s64",
      "userId": "17627097548623080996"
     },
     "user_tz": 300
    },
    "id": "5r_h38uj-WL4",
    "outputId": "0d59963a-85f0-4401-d2a3-83c6a637d6b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "subsequent-tonight",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1644369572774,
     "user": {
      "displayName": "Aditya Sidharta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj-wonlf58m1hUVaWshoARe7aHldD1aYfVlSHcyArA=s64",
      "userId": "17627097548623080996"
     },
     "user_tz": 300
    },
    "id": "subsequent-tonight",
    "outputId": "42ec0bdc-e08b-44de-a8d0-f47991ce7a31"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.06476289, 0.05841569, 0.07252933, 0.05378964, 0.08867795,\n",
       "        0.        , 0.11137269, 0.        , 0.14325408, 0.24628655,\n",
       "        0.29886952, 0.        , 0.        , 0.37915415, 0.63865133,\n",
       "        0.        ]), array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_pi_s, p_pi_s = policy_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)\n",
    "V_pi_s, p_pi_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5DvU69mr9BNo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1644369574561,
     "user": {
      "displayName": "Aditya Sidharta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj-wonlf58m1hUVaWshoARe7aHldD1aYfVlSHcyArA=s64",
      "userId": "17627097548623080996"
     },
     "user_tz": 300
    },
    "id": "5DvU69mr9BNo",
    "outputId": "24429244-e76b-424f-e1da-39ee21bde113"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_policy(p_pi_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "V5jt-1Cd-Vy7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 165,
     "status": "ok",
     "timestamp": 1644369575601,
     "user": {
      "displayName": "Aditya Sidharta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj-wonlf58m1hUVaWshoARe7aHldD1aYfVlSHcyArA=s64",
      "userId": "17627097548623080996"
     },
     "user_tz": 300
    },
    "id": "V5jt-1Cd-Vy7",
    "outputId": "e66a9443-ba69-4073-9737-b6f3ae8d76fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['left', 'up', 'left', 'up']\n",
      "['left', 'left', 'left', 'left']\n",
      "['up', 'down', 'left', 'left']\n",
      "['left', 'right', 'down', 'left']\n"
     ]
    }
   ],
   "source": [
    "visualize_policy(p_pi_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "military-presence",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 182,
     "status": "ok",
     "timestamp": 1644369576768,
     "user": {
      "displayName": "Aditya Sidharta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj-wonlf58m1hUVaWshoARe7aHldD1aYfVlSHcyArA=s64",
      "userId": "17627097548623080996"
     },
     "user_tz": 300
    },
    "id": "military-presence",
    "outputId": "3b467b84-469c-4360-9d3d-a072728a27f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.00069547, 0.00139264, 0.00562142, 0.00171616, 0.00280064,\n",
       "        0.        , 0.02169924, 0.        , 0.01214942, 0.04755381,\n",
       "        0.1032758 , 0.        , 0.        , 0.12348492, 0.44745551,\n",
       "        0.        ]), array([1, 3, 2, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_pi_f, p_pi_f = policy_iteration(env.P, env.nS, env.nA, gamma=0.6, tol=1e-3)\n",
    "V_pi_f, p_pi_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "AgJlzEYL9Fg9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1644369577628,
     "user": {
      "displayName": "Aditya Sidharta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj-wonlf58m1hUVaWshoARe7aHldD1aYfVlSHcyArA=s64",
      "userId": "17627097548623080996"
     },
     "user_tz": 300
    },
    "id": "AgJlzEYL9Fg9",
    "outputId": "9241e467-0350-4e3c-ec88-98d449c7b332"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_policy(p_pi_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9Zy-6Nyg9cQd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 175,
     "status": "ok",
     "timestamp": 1644369578697,
     "user": {
      "displayName": "Aditya Sidharta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj-wonlf58m1hUVaWshoARe7aHldD1aYfVlSHcyArA=s64",
      "userId": "17627097548623080996"
     },
     "user_tz": 300
    },
    "id": "9Zy-6Nyg9cQd",
    "outputId": "2d32a81d-6a5c-424e-cde2-689c8a5b03a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['down', 'up', 'right', 'up']\n",
      "['left', 'left', 'left', 'left']\n",
      "['up', 'down', 'left', 'left']\n",
      "['left', 'right', 'down', 'left']\n"
     ]
    }
   ],
   "source": [
    "visualize_policy(p_pi_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-unknown",
   "metadata": {
    "id": "fifth-unknown"
   },
   "source": [
    "#### What do you observe in terms of impact of gamma on the actions taken by the policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GGe3wwuEoKye",
   "metadata": {
    "id": "GGe3wwuEoKye"
   },
   "source": [
    "*YOUR ANSWER*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H5dn4TsZ_9r3",
   "metadata": {
    "id": "H5dn4TsZ_9r3"
   },
   "source": [
    "Discount factor / gamma causes the Value Iteration to place more emphasis in the current rewards as opposed to the future rewards. Therefore, this causes the policy to be more aggressive in reaching the goal."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_1_Frozen_Lake_Part_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
